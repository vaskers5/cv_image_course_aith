{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kazanplova/anaconda3/envs/flux_train/lib/python3.11/site-packages/tqdm/auto.py:21: TqdmWarning: IProgress not found. Please update jupyter and ipywidgets. See https://ipywidgets.readthedocs.io/en/stable/user_install.html\n",
      "  from .autonotebook import tqdm as notebook_tqdm\n"
     ]
    }
   ],
   "source": [
    "import torch\n",
    "import torch.nn as nn\n",
    "from torch.utils.data import DataLoader\n",
    "from torchvision import datasets, transforms\n",
    "from transformers import ViTModel, AutoModel, AutoFeatureExtractor, AutoTokenizer\n",
    "from sklearn.metrics import accuracy_score, precision_score, recall_score, f1_score\n",
    "from tqdm import tqdm\n",
    "import aim\n",
    "import torch.optim as optim\n",
    "\n",
    "class CustomSigLIP(nn.Module):\n",
    "    def __init__(self, vision_model_name, text_model_name, projection_dim=512):\n",
    "        super(CustomSigLIP, self).__init__()\n",
    "\n",
    "        # Load Vision Transformer (ViT) for image encoding\n",
    "        self.vision_model = ViTModel.from_pretrained(vision_model_name)\n",
    "        vision_config = self.vision_model.config\n",
    "        self.image_projection = nn.Linear(vision_config.hidden_size, projection_dim)\n",
    "\n",
    "        # Load Text Transformer (e.g., BERT) for text encoding\n",
    "        self.text_model = AutoModel.from_pretrained(text_model_name)\n",
    "        text_config = self.text_model.config\n",
    "        self.text_projection = nn.Linear(text_config.hidden_size, projection_dim)\n",
    "\n",
    "        # Logit scaling parameter\n",
    "        self.logit_scale = nn.Parameter(torch.ones([]) * torch.log(torch.tensor(1 / 0.07)))\n",
    "\n",
    "    def forward(self, images, text):\n",
    "        # Encode images\n",
    "        image_features = self.vision_model(pixel_values=images).pooler_output\n",
    "        image_features = self.image_projection(image_features)\n",
    "        image_features = nn.functional.normalize(image_features, dim=-1)\n",
    "\n",
    "        # Encode text\n",
    "        text_features = self.text_model(**text).pooler_output\n",
    "        text_features = self.text_projection(text_features)\n",
    "        text_features = nn.functional.normalize(text_features, dim=-1)\n",
    "\n",
    "        # Compute similarity logits\n",
    "        logit_scale = self.logit_scale.exp()\n",
    "        logits = logit_scale * torch.matmul(image_features, text_features.t())\n",
    "        return logits\n",
    "\n",
    "    def encode_image(self, images):\n",
    "        # Encode images only\n",
    "        image_features = self.vision_model(pixel_values=images).pooler_output\n",
    "        image_features = self.image_projection(image_features)\n",
    "        return nn.functional.normalize(image_features, dim=-1)\n",
    "\n",
    "    def encode_text(self, text):\n",
    "        # Encode text only\n",
    "        text_features = self.text_model(**text).pooler_output\n",
    "        text_features = self.text_projection(text_features)\n",
    "        return nn.functional.normalize(text_features, dim=-1)\n",
    "\n",
    "\n",
    "# Helper function to load the custom model\n",
    "def load_custom_siglip(vision_model_name=\"google/vit-base-patch16-224-in21k\",\n",
    "                       text_model_name=\"bert-base-uncased\",\n",
    "                       projection_dim=512):\n",
    "    return CustomSigLIP(vision_model_name, text_model_name, projection_dim)\n",
    "\n",
    "\n",
    "def compute_metrics(predictions, targets):\n",
    "    \"\"\"\n",
    "    Compute multi-class classification metrics.\n",
    "    Args:\n",
    "        predictions: Tensor of predicted class indices.\n",
    "        targets: Tensor of ground truth class indices.\n",
    "    Returns:\n",
    "        dict: Metrics (accuracy, precision, recall, F1-score).\n",
    "    \"\"\"\n",
    "    predictions = predictions.cpu().numpy()\n",
    "    targets = targets.cpu().numpy()\n",
    "\n",
    "    metrics = {\n",
    "        \"accuracy\": accuracy_score(targets, predictions),\n",
    "        \"precision\": precision_score(targets, predictions, average=\"weighted\", zero_division=0),\n",
    "        \"recall\": recall_score(targets, predictions, average=\"weighted\", zero_division=0),\n",
    "        \"f1_score\": f1_score(targets, predictions, average=\"weighted\", zero_division=0),\n",
    "    }\n",
    "    return metrics\n",
    "\n",
    "\n",
    "# Parameters\n",
    "vision_model_name = \"google/vit-base-patch16-224-in21k\"\n",
    "text_model_name = \"bert-base-uncased\"\n",
    "projection_dim = 512\n",
    "batch_size = 32\n",
    "num_epochs = 5\n",
    "learning_rate = 1e-5\n",
    "\n",
    "device = torch.device(\"cuda\" if torch.cuda.is_available() else \"cpu\")\n",
    "\n",
    "#"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Data preparation\n",
    "train_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),  # Resize to 224x224 (ViT input size)\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])  # ImageNet normalization\n",
    "])\n",
    "\n",
    "val_transform = transforms.Compose([\n",
    "    transforms.Resize((224, 224)),\n",
    "    transforms.ToTensor(),\n",
    "    transforms.Normalize(mean=[0.485, 0.456, 0.406], std=[0.229, 0.224, 0.225])\n",
    "])\n",
    "\n",
    "train_dataset = datasets.Flowers102(root=\"./data\", split=\"train\", transform=train_transform, download=True)\n",
    "val_dataset = datasets.Flowers102(root=\"./data\", split=\"val\", transform=val_transform, download=True)\n",
    "test_dataset = datasets.Flowers102(root=\"./data\", split=\"test\", transform=val_transform, download=True)\n",
    "\n",
    "\n",
    "train_loader = DataLoader(train_dataset, batch_size=batch_size, shuffle=True)\n",
    "val_loader = DataLoader(val_dataset, batch_size=batch_size, shuffle=False)\n",
    "test_loader = DataLoader(test_dataset, batch_size=batch_size, shuffle=False)\n",
    "\n",
    "# Load SigLIP model\n",
    "model = CustomSigLIP(vision_model_name, text_model_name, projection_dim)\n",
    "model.to(device)\n",
    "\n",
    "label_names = [item.strip().strip(\"'\").strip('\"') for item in open('flower_labels.txt').read().split('\\n')]\n",
    "labels_to_texts = {idx: name for idx, name in enumerate(label_names)}\n",
    "\n",
    "\n",
    "# Initialize Aim tracker\n",
    "run = aim.Run(repo='.', experiment='multi_class_classification_flowers102')\n",
    "run[\"hparams\"] = {\n",
    "    \"vision_model_name\": vision_model_name,\n",
    "    \"text_model_name\": text_model_name,\n",
    "    \"projection_dim\": projection_dim,\n",
    "    \"batch_size\": batch_size,\n",
    "    \"num_epochs\": num_epochs,\n",
    "    \"learning_rate\": learning_rate,\n",
    "}"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Training loop\n",
    "def train_with_logging(model, train_loader, labels_to_texts, tokenizer, optimizer, criterion, device, num_epochs, val_loader=None):\n",
    "    model.train()\n",
    "\n",
    "    for epoch in range(num_epochs):\n",
    "        total_loss = 0.0\n",
    "        all_predictions = []\n",
    "        all_targets = []\n",
    "\n",
    "        for images, labels in tqdm(train_loader):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Generate text descriptions for the labels\n",
    "            text_descriptions = [labels_to_texts[label.item()][0] for label in labels]\n",
    "            encoded_texts = tokenizer(text_descriptions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(images, encoded_texts)\n",
    "\n",
    "            # Compute labels for cross-entropy loss (diagonal matrix)\n",
    "            targets = torch.arange(len(images)).to(device)\n",
    "            loss = criterion(logits, targets)\n",
    "\n",
    "            # Backward pass\n",
    "            optimizer.zero_grad()\n",
    "            loss.backward()\n",
    "            optimizer.step()\n",
    "\n",
    "            # Accumulate loss\n",
    "            total_loss += loss.item()\n",
    "\n",
    "            # Collect predictions and targets for metrics\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            all_predictions.append(predictions)\n",
    "            all_targets.append(targets)\n",
    "\n",
    "        # Concatenate all predictions and targets\n",
    "        all_predictions = torch.cat(all_predictions, dim=0)\n",
    "        all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "        # Compute metrics\n",
    "        metrics = compute_metrics(all_predictions, all_targets)\n",
    "\n",
    "        # Log metrics and loss to Aim\n",
    "        run.track(total_loss / len(train_loader), name='train_loss', step=epoch, context={\"subset\":\"train\"})\n",
    "        for metric_name, metric_value in metrics.items():\n",
    "            run.track(metric_value, metric_name, step=epoch, context={\"subset\":\"train\"})\n",
    "\n",
    "        # Print epoch summary\n",
    "        print(f\"Epoch [{epoch+1}/{num_epochs}], Loss: {total_loss / len(train_loader):.4f}\")\n",
    "        print(f\"Train Metrics: {metrics}\")\n",
    "\n",
    "        # Validation loop (if val_loader is provided)\n",
    "        if val_loader:\n",
    "            val_metrics = test_with_logging(model, val_loader, labels_to_texts, tokenizer, device, log_prefix=\"val\")\n",
    "            print(f\"Validation Metrics: {val_metrics}\")\n",
    "\n",
    "\n",
    "def test_with_logging(model, test_loader, labels_to_texts, tokenizer, device, log_prefix=\"test\"):\n",
    "    model.eval()\n",
    "    all_predictions = []\n",
    "    all_targets = []\n",
    "\n",
    "    with torch.no_grad():\n",
    "        for images, labels in tqdm(test_loader):\n",
    "            # Move data to device\n",
    "            images = images.to(device)\n",
    "            labels = labels.to(device)\n",
    "\n",
    "            # Generate text descriptions for the labels\n",
    "            text_descriptions = [labels_to_texts[label.item()][0] for label in labels]\n",
    "            encoded_texts = tokenizer(text_descriptions, return_tensors=\"pt\", padding=True, truncation=True).to(device)\n",
    "\n",
    "            # Forward pass\n",
    "            logits = model(images, encoded_texts)\n",
    "\n",
    "            # Collect predictions and targets\n",
    "            predictions = torch.argmax(logits, dim=1)\n",
    "            all_predictions.append(predictions)\n",
    "            all_targets.append(torch.arange(len(labels)).to(device))  # Target is the diagonal (index matches)\n",
    "\n",
    "    # Concatenate all predictions and targets\n",
    "    all_predictions = torch.cat(all_predictions, dim=0)\n",
    "    all_targets = torch.cat(all_targets, dim=0)\n",
    "\n",
    "    # Compute metrics\n",
    "    metrics = compute_metrics(all_predictions, all_targets)\n",
    "\n",
    "    # Log metrics to Aim\n",
    "    for metric_name, metric_value in metrics.items():\n",
    "        run.track(metric_value, name=metric_name, context={\"subset\":log_prefix})\n",
    "\n",
    "    return metrics\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/data/kazanplova/anaconda3/envs/flux_train/lib/python3.11/site-packages/transformers/models/vit/feature_extraction_vit.py:28: FutureWarning: The class ViTFeatureExtractor is deprecated and will be removed in version 5 of Transformers. Please use ViTImageProcessor instead.\n",
      "  warnings.warn(\n",
      "/data/kazanplova/anaconda3/envs/flux_train/lib/python3.11/site-packages/transformers/tokenization_utils_base.py:1601: FutureWarning: `clean_up_tokenization_spaces` was not set. It will be set to `True` by default. This behavior will be depracted in transformers v4.45, and will be then set to `False` by default. For more details check this issue: https://github.com/huggingface/transformers/issues/31884\n",
      "  warnings.warn(\n",
      "100%|██████████| 32/32 [00:13<00:00,  2.41it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [1/5], Loss: 3.4169\n",
      "Train Metrics: {'accuracy': 0.05196078431372549, 'precision': 0.0476224736571144, 'recall': 0.05196078431372549, 'f1_score': 0.04835113497085057}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:07<00:00,  4.39it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: {'accuracy': 0.06470588235294118, 'precision': 0.03180562637670194, 'recall': 0.06470588235294118, 'f1_score': 0.0391933441254895}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:12<00:00,  2.67it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch [2/5], Loss: 2.9995\n",
      "Train Metrics: {'accuracy': 0.1803921568627451, 'precision': 0.21347992340882366, 'recall': 0.1803921568627451, 'f1_score': 0.16676826429080457}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "100%|██████████| 32/32 [00:07<00:00,  4.10it/s]\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Validation Metrics: {'accuracy': 0.09313725490196079, 'precision': 0.04867427357739729, 'recall': 0.09313725490196079, 'f1_score': 0.05656420490982364}\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      " 47%|████▋     | 15/32 [00:05<00:06,  2.48it/s]"
     ]
    }
   ],
   "source": [
    "# Load feature extractor and tokenizer\n",
    "feature_extractor = AutoFeatureExtractor.from_pretrained(vision_model_name)\n",
    "tokenizer = AutoTokenizer.from_pretrained(text_model_name)\n",
    "\n",
    "# Define optimizer and loss function\n",
    "optimizer = optim.AdamW(model.parameters(), lr=learning_rate, weight_decay=0.01)\n",
    "criterion = nn.CrossEntropyLoss()\n",
    "\n",
    "# Train the model\n",
    "train_with_logging(model, train_loader, labels_to_texts, tokenizer, optimizer, criterion, device, num_epochs, val_loader)\n",
    "\n",
    "# Test the model\n",
    "test_metrics = test_with_logging(model, test_loader, labels_to_texts, tokenizer, device)\n",
    "print(f\"Test Metrics: {test_metrics}\")\n",
    "\n",
    "run.close()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "flux_train",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
